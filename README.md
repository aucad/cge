[![Build](https://github.com/aucad/new-experiments/actions/workflows/build.yml/badge.svg)](https://github.com/aucad/new-experiments/actions/workflows/build.yml)

# Constraint guaranteed evasion attacks

**We present an approach to introduce constraints to unconstrained adversarial machine learning evasion attacks.
The technique is founded on a constraint validation algorithm, _Contraint Guaranteed Evasion_ (CGE), that guarantees generated adversarial examples also satisfy domain constraints.**

This repository includes a full CGE implemenation, and an experimental setup for running various adversarial evasion attacks, enhanced with CGE, on different data sets and victim classifiers.

**Experiment options**

- **Attacks**: Projected Gradient Descent (PGD), Zeroth-Order Optimization (ZOO), HopSkipJump attack. 
- **Classifiers**: Keras deep neural network and tree-based ensemble XGBoost.
- **Data sets**: Four different data sets from various domains.
- **Constraints**: Constraints are configurable experiment inputs and config files show how to specify them.
- **Comparison attack**: _Constrained Projected Gradient Descent_ (C-PGD) by [Simonetto et al](https://arxiv.org/abs/2112.01156).

**Data sets**

- [**IoT-23**](https://doi.org/10.5281/zenodo.4743746) - Malicious and benign IoT network traffic; 10,000 rows, 2 classes (sampled).
- [**UNSW-NB15**](https://doi.org/10.1109/MilCIS.2015.7348942) - Network intrusion dataset with 9 attacks; 10,000 rows, 2 classes (sampled). 
- [**URL**](https://doi.org/10.1016/j.engappai.2021.104347) - Legitimate and phishing URLs; 11,430 rows, 2 classes (full data, not sampled).
- [**LCLD**](https://www.kaggle.com/datasets/wordsforthewise/lending-club) - Kaggle's All Lending Club loan data; 20,000 rows, 2 classes (sampled).

<details>
<summary>Notes on preprocessing and sampling</summary>
<ul>
<li>The input data must be numeric and parse to a numeric type.</li>
<li>Categorical attributes must be one-hot encoded.</li>
<li>Data should not be normalized (otherwise constraints must include manual scaling).</li>
<li>All data sets have 50/50 class label distribution.</li>
<li>The provided sampled data sets were generated by <a href="https://waikato.github.io/weka-blog/posts/2019-01-30-sampling/" target="_blank">random sampling without replacement</a>.</li>
</ul>
</details>

### Experiment workflow

A single experiment consists of:

1. configration, data preprocessing, and setup
2. training a classification model on a choice data set
3. applying an adversarial attack against the victim model 
4. scoring the adversarial examples, and
5. recording the result.
   
Steps 2-4 are repeated k times, for k-folds of input data.
In step 3, a constraint-validation approach can be enabled, impacting the validity of the generated adversarial examples.

<pre>
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
â—‹â”€â”€â”€â”¤  args-parser  â”œâ”€â”€â”€â”€â”€â”¤     setup     â”œâ”€â”€â”€â”€â”€â”¤      run      â”œâ”€â”€â”€â”€â”€â”¤      end      â”œâ”€â”€â”€â—
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     inputs:              * preprocess data      k times:                write result
     * data set           * init classifier      1. train model     
     * constraints        * init attack          2. attack
     * other configs      * init validation      3. score
</pre>

### The CGE algorithm

* Complete implementation of the CGE algorithm is in [`cge`](https://github.com/aucad/cge/tree/main/cge) directory.
* Examples of (static) constraint definitions are in [`config`](https://github.com/aucad/cge/blob/main/config/iot23.yaml).
* Constraints are converted to executable form using a [preprocessor](https://github.com/aucad/cge/blob/main/exp/preproc.py#L14-L27).
* Examples of integrating CGE to evasion attacks: [example 1](https://github.com/aucad/cge/blob/main/exp/hopskip.py#L26-L28), [example 2](https://github.com/aucad/cge/blob/main/exp/pgd.py#L44), [example 3](https://github.com/aucad/cge/blob/main/exp/zoo.py#L44).

### Repository organization

<pre>
.
â”œâ”€ .github/           Automated workflows, development instructions
â”œâ”€ cge/               CGE algorithm implementation   
â”œâ”€ comparison/        C-PGD attack implementation and its license
â”œâ”€ config/            Experiment configuration files 
â”œâ”€ data/              
â”‚  â”œâ”€ feature_*.csv   C-PGD feature files
â”‚  â””â”€ *               Preprocessed input data 
â”œâ”€ exp/               Source code for running experiments
â”œâ”€ plot/              Plotting of experiment results
â”œâ”€ ref_result/        Referential result for comparison
â”œâ”€ test/              Unit tests (for development) 
â”œâ”€ LICENSE          
â”œâ”€ Makefile           Pre-configured commands to ease running experiments
â””â”€ requirements.txt   Software dependencies
</pre>

<br/>

## âœ´ï¸ Reproducing paper experiments

**Software requirements**

* [Python](https://www.python.org/downloads/) -- version 3.9 or higher
* [GNU make](https://www.gnu.org/software/make/manual/make.html) -- version 3.81 or later

Check your environment using the following command, and install/upgrade as necessary.

```
python3 --version & make --version
```

**Install dependencies**

```
pip install -r requirements.txt --user
```

### ğŸ“ Repeating evaluations

`â±ï¸ 24â€”48h` **Run attack evaluations.**   
Run experiments for all supported combinations of data sets $\times$ classifiers $\times$ attacks (20 experiment cases). 

<pre>
make attacks   -- run all attacks, using constraint enforcement.
make original  -- run all attacks, but without validation (ignore constraints).
</pre>

`â±ï¸ 30 minâ€”3 h` **Run constraint performance test.**   
Uses increasing number of constraints and increasing complexity of constraints, to measure performance impact of introducing constraints to an attack. 
This experiment runs PGD and CPGD and VPGD attacks on a neural network classifier trained on UNSW-NB15 data set.

```
make perf
```

### ğŸ“Š Visualizing

**Plots.** Generate plots of experiment results.

```
make plots
```

**Comparison plot.** To plot results from some other directory, e.g. `ref_result`, append a directory name.

```
make plots DIR=ref_result
```

<br/>

## Extended use

The default experiment options are defined statically.
An experiment run can be customized further with command line arguments, to override the static options.
To run custom experiments, call the `exp` module directly.

```
python3 -m exp [PATH] {ARGS}
```

For a list of supported arguments, run:

```
python3 -m exp --help
```

All plotting utilities live separately from experiments in `plot` module.
For plotting help, run:

```
python3 -m plot --help
```
